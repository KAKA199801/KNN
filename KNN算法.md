

# KNN算法

​      KNN算法属于监督学习范畴，是一种基于带有标签的训练集数据与测试集数据的距离远近实现的算法。首先先计算出测试集的数据与训练集的各数据的距离，然后选取距离最小的前k个训练集的数据，根据这k个训练集的数据的标签分类，取它们所属最多的类别即为测试集数据所属的类别，从而达到分类目的。

应用KNN的常见问题

（1）k值设定为多大？ 

k太小，分类结果易受噪声的影响；k太大，近邻中又可能包含太多的其它类别的点。（对距离加权，可以降低k值设定的影响）。

k值通常是采用交叉检验来确定（以k=1为基准）。

经验规则：k一般低于训练样本数的平方根

（2）类别如何判定最合适？

投票法没有考虑近邻的距离的远近，距离更近的近邻也许更应该决定最终的分类，所以加权投票法更恰当一些。

（3）如何选择合适的距离衡量？

高维度对距离衡量的影响：众所周知当变量数越多，欧式距离的区分能力就越差。

变量值域对距离的影响：值域越大的变量常常会在距离计算中占据主导作用，因此应先对变量进行标准化。

（4）训练样本是否要一视同仁？ 

在训练集中，有些样本可能是更值得依赖的。 

可以给不同的样本施加不同的权重，加强依赖样本的权重，降低不可信赖样本的影响。（如何判断样本可信不可信）

（5）能否大幅减少训练样本量，同时又保持分类精度？ 

浓缩技术(condensing) 

编辑技术(editing)

- 类别的判断方法

- （1）投票决定：少数服从多数，近邻中哪个类别的点最多就分为该类。

  （2）加权投票法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大。 knn还可用于回归，目标样本的属性值是k个邻居属性值的平均值。